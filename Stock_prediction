{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9294945,"sourceType":"datasetVersion","datasetId":5627486}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/adarshsudheer/machine-learning-project-2?scriptVersionId=213570785\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Stock Prediction on Reliance dataset\r\n\r\n## Overview\r\nThis script is designed to load pre-trained word embeddings using Gensim's `KeyedVectors`, extract vectors for specific keys, and prepare the data for further analysis or modeling. It is tailored for embeddings related to the \"Reliance\" dataset and operates within a Kaggle Notebook environment.\r\n\r\n---\r\n\r\n## Dependencies\r\nEnsure the following Python libraries are installed:\r\n\r\n1. `gensim`: For handling word embeddings.\r\n2. `numpy`: For numerical computations (imported but not yet used in this snippet).\r\n\r\nYou can install them using the following commands if necessary:\r\n```bash\r\n!pip install gensim\r\n!pip install numpy\r\n```\r\n\r\n---\r\n\r\n## Script Details\r\n\r\n### 1. **Setting the Script Name**\r\n```python\r\nscript_name = \"Reliance\"\r\n```\r\n- Defines the script name for documentation or logging purposes.\r\n- The variable `script_name` is set to \"Reliance\" to indicate the dataset or purpose of the script.\r\n\r\n### 2. **Loading Word Embeddings**\r\n```python\r\nlink = \"/kaggle/input/trading-dataset/Machine Learning Assignment Dataset/Reliance_embeddings.kv\"\r\nword_vectors = KeyedVectors.load(link, mmap='r')\r\n```\r\n- **Purpose**: Load the pre-trained word embeddings stored in the `.kv` format.\r\n- **Parameters**:\r\n  - `link`: Path to the `.kv` file containing the embeddings. Update this path if the embeddings are stored in a different location.\r\n  - `mmap='r'`: Memory-mapping mode for efficient loading of large files in read-only mode.\r\n- **Output**: A `KeyedVectors` object, which provides access to the embeddings.\r\n\r\n### 3. **Retrieving Metadata (Vector Keys)**\r\n```python\r\nmetadata = word_vectors.index_to_key\r\n```\r\n- **Purpose**: Extract all the keys (words or tokens) present in the loaded embeddings.\r\n- **Output**: A list of keys, which can be words, phrasesint(key, word_vectors[key])  # Print the key and its vector\r\n```\r\n\r\n---\r\n\r\n## Usage\r\n1. Ensure the embedding file (`Reliance_embeddings.kv`) is uploaded to the specified path in the Kaggle environment.\r\n2. Run the script to load the embeddings and extract their metadata.\r\n3. Modify the `link` variable if the file is located elsewhere.\r\n4. Use the vectors stored in `W` for downstream tasks like clustering, visualization, or machine learning.\r\n\r\n---\r\n\r\n## Notes\r\n- **Scalability**: If the embeddings are large, consider optimizing memory usage by processing keys in batches.\r\n- **Error Handling**: Ensure the file exists at the specified path; otherwise, a `FileNotFoundError` will occur.\r\n- **Potential Extensions**:\r\n  - Use `numpy` to perform operations on the vector list `W`.\r\n  - Visualize embeddings using techniques like PCA or t-SNE.\r\n  - Builion**: Ensure adequate memory is available or use `mmap` to load embeddings in read-only mode.\r\n\r\n---\r\n\r\nThis documentation serves as a guide for using the script within a Kaggle Notebook for embedding-related tasks.\r\n\r\n","metadata":{}},{"cell_type":"code","source":"\nfrom gensim.models import KeyedVectors\nimport numpy as np\n# Setting the script name.\nscript_name = \"Reliance\"\n\n# Load the word embeddings\nlink = \"/kaggle/input/trading-dataset/Machine Learning Assignment Dataset/Reliance_embeddings.kv\"\nword_vectors = KeyedVectors.load(link, mmap='r')\n\n# Getting the vector keys (or filenames in this case)\nmetadata = word_vectors.index_to_key\nW=[]\n# Print the first 5 keys and their associated vectors\nfor key in metadata:  # Adjust the range as needed to print more or fewer keys\n    W.append(word_vectors[key])\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:55:18.68976Z","iopub.execute_input":"2024-12-17T18:55:18.690155Z","iopub.status.idle":"2024-12-17T18:55:18.80289Z","shell.execute_reply.started":"2024-12-17T18:55:18.690125Z","shell.execute_reply":"2024-12-17T18:55:18.802234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt  # Corrected import\n%matplotlib inline\n\n# Load CSV\nload_csv = pd.read_csv(\"/kaggle/input/trading-dataset/Machine Learning Assignment Dataset/Reliance.csv\")\ndf = pd.DataFrame(load_csv)\ndf['date'] = pd.to_datetime(df['date'])\n\ndf\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:55:18.804165Z","iopub.execute_input":"2024-12-17T18:55:18.804418Z","iopub.status.idle":"2024-12-17T18:55:22.989727Z","shell.execute_reply.started":"2024-12-17T18:55:18.804393Z","shell.execute_reply":"2024-12-17T18:55:22.988738Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the 'close' column over time\nplt.figure(figsize=(15, 6))  # Optional: Set figure size for better visualization\nplt.title(\"Close Price Over Time\")  # Add title to the plot\nplt.xlabel(\"Date\")  # Label for X-axis\nplt.ylabel(\"Close Price\")  # Label for Y-axis\nplt.plot(df[\"date\"], df['close'], color='blue', linewidth=1)  # Use the date directly for x-axis\n\nplt.show()  # Display the plot","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:55:22.99084Z","iopub.execute_input":"2024-12-17T18:55:22.991125Z","iopub.status.idle":"2024-12-17T18:55:29.638549Z","shell.execute_reply.started":"2024-12-17T18:55:22.991099Z","shell.execute_reply":"2024-12-17T18:55:29.637767Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install pandas_ta","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:55:29.640905Z","iopub.execute_input":"2024-12-17T18:55:29.641275Z","iopub.status.idle":"2024-12-17T18:55:38.628197Z","shell.execute_reply.started":"2024-12-17T18:55:29.641237Z","shell.execute_reply":"2024-12-17T18:55:38.627066Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Notebook Documentation: Stock Price Analysis with Technical Indicators\n---\nThis notebook focuses on analyzing stock price movements by incorporating technical indicators using the pandas_ta library. The dataset, represented as Stock_price, contains historical stock price data with at least the close column for closing prices. Two key indicators are computed: Relative Strength Index (RSI) and Exponential Moving Average (EMA), added as new columns. The RSI helps identify overbought or oversold conditions, while the EMA smooths price data to track trends effectively. Additionally, the notebook prepares a target feature for predicting stock price trends. This target is a binary indicator showing whether the current closing price is higher than the previous day's closing price, computed using a rolling window function. A simplified dataset is prepared with the renamed column actual close and the newly created Target for further modeling or analysis. \n\n---","metadata":{}},{"cell_type":"code","source":"import pandas_ta as ta\nStock_price=df\nStock_price\nStock_price['rsi'] = ta.rsi(Stock_price.close)\nStock_price['ema'] = ta.ema(Stock_price.close)\n\ndata= Stock_price[['close']]\ndata=data.rename(columns={'close':\"actual close\"})\ndata['Target'] = Stock_price['close'].rolling(2).apply(lambda x: x.iloc[1] > x.iloc[0])\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:55:38.629519Z","iopub.execute_input":"2024-12-17T18:55:38.629797Z","iopub.status.idle":"2024-12-17T18:55:54.929069Z","shell.execute_reply.started":"2024-12-17T18:55:38.62977Z","shell.execute_reply":"2024-12-17T18:55:54.928338Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"A shifted copy of the selected columns, copy_data, is prepared using .shift(1), effectively aligning the dataset for lagged feature analysis. This step introduces a one-day lag in the data, enabling the model to learn dependencies between the previous day’s data and subsequent outcomes.","metadata":{}},{"cell_type":"code","source":"cols=['open','high','close','rsi','ema','low','volume','date']\n\ncopy_data = Stock_price[cols].copy().shift(1)\n\ndata= data.join(copy_data)\ndata = data.iloc[1:]\nStock_price=data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:55:54.930154Z","iopub.execute_input":"2024-12-17T18:55:54.930426Z","iopub.status.idle":"2024-12-17T18:55:55.03577Z","shell.execute_reply.started":"2024-12-17T18:55:54.9304Z","shell.execute_reply":"2024-12-17T18:55:55.035084Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nStock_price=Stock_price.dropna()\nStock_price","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:55:55.036968Z","iopub.execute_input":"2024-12-17T18:55:55.037562Z","iopub.status.idle":"2024-12-17T18:55:55.077649Z","shell.execute_reply.started":"2024-12-17T18:55:55.03752Z","shell.execute_reply":"2024-12-17T18:55:55.0768Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nplt.plot(Stock_price['date'],Stock_price['close'],color='blue')\nplt.plot(Stock_price['date'],Stock_price['actual close'],color='red')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:55:55.078696Z","iopub.execute_input":"2024-12-17T18:55:55.078963Z","iopub.status.idle":"2024-12-17T18:56:07.399067Z","shell.execute_reply.started":"2024-12-17T18:55:55.078939Z","shell.execute_reply":"2024-12-17T18:56:07.398185Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here is the stock prices along the data time","metadata":{}},{"cell_type":"code","source":"Input_data = Stock_price.drop(columns=['date'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:56:07.400309Z","iopub.execute_input":"2024-12-17T18:56:07.400668Z","iopub.status.idle":"2024-12-17T18:56:07.420765Z","shell.execute_reply.started":"2024-12-17T18:56:07.40062Z","shell.execute_reply":"2024-12-17T18:56:07.419901Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nStock= Stock_price.drop(columns=['date'])\nInput_data = np.array(Stock.iloc[:29979])\nValidation_data = np.array(Stock.iloc[29979:])\nCross_validation = np.array(Stock.iloc[29979:])\nOutput_data = W\nInput_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:56:07.423446Z","iopub.execute_input":"2024-12-17T18:56:07.423698Z","iopub.status.idle":"2024-12-17T18:56:07.505351Z","shell.execute_reply.started":"2024-12-17T18:56:07.423675Z","shell.execute_reply":"2024-12-17T18:56:07.504473Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Applying PCA on the vectors\n\nTo optimize model performance and address the curse of dimensionality, Principal Component Analysis (PCA) is applied to the dataset. PCA is a dimensionality reduction technique that transforms high-dimensional data into a smaller number of uncorrelated components while retaining the maximum possible variance. In this notebook, the PCA class from scikit-learn is used to reduce the dataset (Output_data) to six principal components (n_components=6). This step helps simplify the dataset, reduce noise, and improve computational efficiency for subsequent modeling tasks.","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=6)  # Reduce to 6 components\n\nOutput_data=pca.fit_transform(Output_data)\n\nOutput_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:56:07.506652Z","iopub.execute_input":"2024-12-17T18:56:07.506939Z","iopub.status.idle":"2024-12-17T18:56:08.328865Z","shell.execute_reply.started":"2024-12-17T18:56:07.506914Z","shell.execute_reply":"2024-12-17T18:56:08.32775Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport xgboost as xgb\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n\n\nn_samples = 29979\nX = Input_data  # OHLC data (replace with actual OHLC data)\ny = Output_data  \n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the XGBoost Regressor\nxgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, max_depth=5, learning_rate=0.1)\n\n# Wrap XGBoost with MultiOutputRegressor to handle multi-output regression\nmulti_output_model = MultiOutputRegressor(xgb_model)\n\nmulti_output_model.fit(X,y)\n\ny_pred = multi_output_model.predict(X_test)\n\n# Evaluate the model performance using Mean Squared Error\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"Mean Squared Error: {mse}\")\n\n# Predict vectors for new OHLC records (replace `new_ohlc_data` with actual data)\nnew_ohlc_data = Validation_data  # Example data (replace with actual new OHLC data)\npredicted_vectors = multi_output_model.predict(new_ohlc_data)\n\npredicted_vectors","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:56:08.330197Z","iopub.execute_input":"2024-12-17T18:56:08.330637Z","iopub.status.idle":"2024-12-17T18:56:11.68228Z","shell.execute_reply.started":"2024-12-17T18:56:08.330587Z","shell.execute_reply":"2024-12-17T18:56:11.681579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Concatenate the new vectors with the given vectors \nprint(np.array(Output_data).shape)\nprint(predicted_vectors.shape)\ncombined_vectors = np.vstack((Output_data,predicted_vectors))\nprint(combined_vectors.shape)\nprint(Stock_price.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:56:11.683642Z","iopub.execute_input":"2024-12-17T18:56:11.683935Z","iopub.status.idle":"2024-12-17T18:56:11.706376Z","shell.execute_reply.started":"2024-12-17T18:56:11.683909Z","shell.execute_reply":"2024-12-17T18:56:11.705459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout\nfrom sklearn.decomposition import PCA\n\n# Assuming input_data is your OHLC and additional data\n# Combine input data with vectors\ncombined_data = np.hstack((Stock_price,combined_vectors))\n\ncols= ['actual close','Target','open','high','close','rsi','ema' ,'low','volume','date','x1','x2','x3','x4','x5','x6']\n\ndf= pd.DataFrame(combined_data,columns=cols)\n\ndf\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:56:11.70761Z","iopub.execute_input":"2024-12-17T18:56:11.707956Z","iopub.status.idle":"2024-12-17T18:56:17.735669Z","shell.execute_reply.started":"2024-12-17T18:56:11.707929Z","shell.execute_reply":"2024-12-17T18:56:17.734785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.to_csv(\"Sample.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:56:17.736789Z","iopub.execute_input":"2024-12-17T18:56:17.737104Z","iopub.status.idle":"2024-12-17T18:56:32.168853Z","shell.execute_reply.started":"2024-12-17T18:56:17.737077Z","shell.execute_reply":"2024-12-17T18:56:32.167872Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df= df.dropna()\n\nX,y = df.drop(columns=['actual close','date']),df['actual close']\n\nX= np.array(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:56:32.17011Z","iopub.execute_input":"2024-12-17T18:56:32.170485Z","iopub.status.idle":"2024-12-17T18:56:32.890188Z","shell.execute_reply.started":"2024-12-17T18:56:32.170449Z","shell.execute_reply":"2024-12-17T18:56:32.889423Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, VotingRegressor\nfrom sklearn.model_selection import cross_val_score\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.metrics import make_scorer, mean_squared_error, r2_score\nimport numpy as np\n\n# Define the models\nlr = LinearRegression()\nxgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)\nlgb_model = lgb.LGBMRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)\nrf = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n\n\nX_train= X[:int(0.7 * len(X))]\ny_train= y[:int(0.7 * len(X))]\nX_test = X[int(0.7 * len(X)):]\ny_test = y[int(0.7 * len(X)):]\n# Create Voting Regressor\nvoting_regressor = VotingRegressor(estimators=[\n    ('lr', lr),\n    ('xgb', xgb_model),\n    ('lgb', lgb_model),\n    ('rf', rf)\n])\n\n# Define models to evaluate\nmodels = {\n    'Linear Regression': lr,\n    'XGBoost': xgb_model,\n    'LightGBM': lgb_model,\n    'Random Forest': rf,\n    'Voting Regressor': voting_regressor\n}\n\n# Perform evaluation for each model\nresults = []\n\nfor name, model in models.items():\n    # Train the model and predict on test set\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    # Calculate metrics\n    mse = mean_squared_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n    \n    results.append({'Model': name, 'Mean Squared Error': mse, 'R² Score': r2})\n\n# Convert results to a DataFrame\nresults_df = pd.DataFrame(results)\n\n# Display the results\nprint(results_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T18:56:32.891224Z","iopub.execute_input":"2024-12-17T18:56:32.891503Z","iopub.status.idle":"2024-12-17T19:02:49.676732Z","shell.execute_reply.started":"2024-12-17T18:56:32.891476Z","shell.execute_reply":"2024-12-17T19:02:49.675652Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T19:02:49.678124Z","iopub.execute_input":"2024-12-17T19:02:49.678493Z","iopub.status.idle":"2024-12-17T19:02:49.688368Z","shell.execute_reply.started":"2024-12-17T19:02:49.678452Z","shell.execute_reply":"2024-12-17T19:02:49.687493Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, mean_absolute_error\n\npredicted_prices=voting_regressor.predict(X)\ndf['predicted_prices'] = predicted_prices\n\n\nmse = mean_squared_error(y, df['predicted_prices'])\n\nr2 = voting_regressor.score(X_test, y_test) \n\n# Display results\nprint(f\"Voting Regressor Performance:\")\nprint(f\"Mean Squared Error: {mse}\")\n\nprint(f\"R² Score: {r2}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T19:02:49.689479Z","iopub.execute_input":"2024-12-17T19:02:49.689757Z","iopub.status.idle":"2024-12-17T19:02:50.230985Z","shell.execute_reply.started":"2024-12-17T19:02:49.689731Z","shell.execute_reply":"2024-12-17T19:02:50.227288Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\n\n\nplt.figure(figsize=(15, 6))  # Optional: Set figure size for better visualization\nplt.title(\"Close Price Over Time V/S Predicted Close Prices\")  # Add title to the plot\nplt.xlabel(\"Date\")  # Label for X-axis\nplt.ylabel(\"Close Price\")  # Label for Y-axis\n\n# Plot predicted closing prices (using test data)\nplt.plot(df['date'],df['predicted_prices'], color='blue', linewidth=1, label='Predicted Close Prices')\n# Set up date formatting for better visualization\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T19:02:50.23308Z","iopub.execute_input":"2024-12-17T19:02:50.233473Z","iopub.status.idle":"2024-12-17T19:02:56.790436Z","shell.execute_reply.started":"2024-12-17T19:02:50.233428Z","shell.execute_reply":"2024-12-17T19:02:56.789492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Lets overlap it with the actual_close price\nplt.figure(figsize=(15, 6))  # Optional: Set figure size for better visualization\nplt.title(\"Close Price Over Time V/S Predicted Close Prices\")  # Add title to the plot\nplt.xlabel(\"Date\")  # Label for X-axis\nplt.ylabel(\"Close Price\")  # Label for Y-axis\n\nplt.plot(df['date'],df['predicted_prices'],color='yellow', linewidth=1, label='Predicted Close Prices')\nplt.plot(df['date'],df['actual close'],color='red')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T19:02:56.791611Z","iopub.execute_input":"2024-12-17T19:02:56.79202Z","iopub.status.idle":"2024-12-17T19:03:08.350331Z","shell.execute_reply.started":"2024-12-17T19:02:56.791984Z","shell.execute_reply":"2024-12-17T19:03:08.349442Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(25, 25))  # Optional: Set figure size for better visualization\nplt.title(\"Close Price Over Time V/S Predicted Close Prices\")  # Add title to the plot\nplt.xlabel(\"Date\")  # Label for X-axis\nplt.ylabel(\"Close Price\")  # Label for Y-axis\nplt.plot(df['date'].loc[0:200],df['predicted_prices'].loc[0:200],color='red', linewidth=1, label='Predicted Close Prices')\nplt.plot(df['date'].loc[0:200],df['actual close'].loc[0:200],color='yellow')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T19:03:08.351416Z","iopub.execute_input":"2024-12-17T19:03:08.351698Z","iopub.status.idle":"2024-12-17T19:03:08.898447Z","shell.execute_reply.started":"2024-12-17T19:03:08.351665Z","shell.execute_reply":"2024-12-17T19:03:08.897556Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! pip install gensim","metadata":{"_kg_hide-output":true,"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T19:03:08.899638Z","iopub.execute_input":"2024-12-17T19:03:08.89999Z","iopub.status.idle":"2024-12-17T19:03:18.211542Z","shell.execute_reply.started":"2024-12-17T19:03:08.899954Z","shell.execute_reply":"2024-12-17T19:03:18.210536Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install backtesting","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T19:03:18.212988Z","iopub.execute_input":"2024-12-17T19:03:18.213267Z","iopub.status.idle":"2024-12-17T19:03:27.322279Z","shell.execute_reply.started":"2024-12-17T19:03:18.21324Z","shell.execute_reply":"2024-12-17T19:03:27.321127Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from backtesting import Strategy\nfrom backtesting import Backtest\n\ndef SIGNAL():\n    return df.TotalSignal\n\nclass MyStrat(Strategy):\n    mysize = 0.1  # Trade size\n    slperc = 0.04\n    tpperc = 0.02\n\n    def init(self):\n        super().init()\n        self.signal1 = self.I(SIGNAL)  # Assuming SIGNAL is a function that returns signals\n\n    def next(self):\n        super().next()\n         \n        if self.signal1 == 2 and not self.position:\n            # Open a new long position with calculated SL and TP\n            current_close = self.data.Close[-1]\n            sl = current_close - self.slperc * current_close  # SL at 4% below the close price\n            tp = current_close + self.tpperc * current_close  # TP at 2% above the close price\n            self.buy(size=self.mysize, sl=sl, tp=tp)\n\n        elif self.signal1 == 1 and not self.position:\n            # Open a new short position, setting SL based on a strategy-specific requirement\n            current_close = self.data.Close[-1]\n            sl = current_close + self.slperc * current_close  # SL at 4% below the close price\n            tp = current_close - self.tpperc * current_close  # TP at 2% above the close price\n            self.sell(size=self.mysize, sl=sl, tp=tp)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T19:03:27.324082Z","iopub.execute_input":"2024-12-17T19:03:27.324509Z","iopub.status.idle":"2024-12-17T19:03:27.333527Z","shell.execute_reply.started":"2024-12-17T19:03:27.324464Z","shell.execute_reply":"2024-12-17T19:03:27.332618Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef total_signal(df, current_candle):\n    current_pos = df.index.get_loc(current_candle)\n    \n    # Buy condition\n    c1 = df['high'].iloc[current_pos] > df['high'].iloc[current_pos-1]\n    c2 = df['high'].iloc[current_pos-1] > df['low'].iloc[current_pos]\n    c3 = df['low'].iloc[current_pos] > df['high'].iloc[current_pos-2]\n    c4 = df['high'].iloc[current_pos-2] > df['low'].iloc[current_pos-1]\n    c5 = df['low'].iloc[current_pos-1] > df['high'].iloc[current_pos-3]\n    c6 = df['high'].iloc[current_pos-3] > df['low'].iloc[current_pos-2]\n    c7 = df['low'].iloc[current_pos-2] > df['low'].iloc[current_pos-3]\n\n    if c1 and c2 and c3 and c4 and c5 and c6 and c7:\n        return 2\n\n    # Symmetrical conditions for short (sell condition)\n    c1 = df['low'].iloc[current_pos] < df['low'].iloc[current_pos-1]\n    c2 = df['low'].iloc[current_pos-1] < df['high'].iloc[current_pos]\n    c3 = df['high'].iloc[current_pos] < df['low'].iloc[current_pos-2]\n    c4 = df['low'].iloc[current_pos-2] < df['high'].iloc[current_pos-1]\n    c5 = df['high'].iloc[current_pos-1] < df['low'].iloc[current_pos-3]\n    c6 = df['low'].iloc[current_pos-3] < df['high'].iloc[current_pos-2]\n    c7 = df['high'].iloc[current_pos-2] < df['high'].iloc[current_pos-3]\n\n    if c1 and c2 and c3 and c4 and c5 and c6 and c7:\n        return 1\n\n    return 0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T19:03:27.334647Z","iopub.execute_input":"2024-12-17T19:03:27.334958Z","iopub.status.idle":"2024-12-17T19:03:27.355421Z","shell.execute_reply.started":"2024-12-17T19:03:27.334933Z","shell.execute_reply":"2024-12-17T19:03:27.354853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_total_signal(df):\n    df['TotalSignal'] = df.apply(lambda row: total_signal(df, row.name), axis=1)\n    return df\n\ndef add_pointpos_column(df, signal_column):\n    \"\"\"\n    Adds a 'pointpos' column to the DataFrame to indicate the position of support and resistance points.\n    \n    Parameters:\n    df (DataFrame): DataFrame containing the stock data with the specified SR column, 'Low', and 'High' columns.\n    sr_column (str): The name of the column to consider for the SR (support/resistance) points.\n    \n    Returns:\n    DataFrame: The original DataFrame with an additional 'pointpos' column.\n    \"\"\"\n    def pointpos(row):\n        if row[signal_column] == 2:\n            return row['low'] - 1e-4\n        elif row[signal_column] == 1:\n            return row['high'] + 1e-4\n        else:\n            return np.nan\n\n    df['pointpos'] = df.apply(lambda row: pointpos(row), axis=1)\n    return df\n\ndef plot_candlestick_with_signals(df, start_index, num_rows):\n    \"\"\"\n    Plots a candlestick chart with signal points.\n    \n    Parameters:\n    df (DataFrame): DataFrame containing the stock data with 'Open', 'High', 'Low', 'Close', and 'pointpos' columns.\n    start_index (int): The starting index for the subset of data to plot.\n    num_rows (int): The number of rows of data to plot.\n    \n    Returns:\n    None\n    \"\"\"\n    df_subset = df[start_index:start_index + num_rows]\n    \n    fig = make_subplots(rows=1, cols=1)\n    \n    fig.add_trace(go.Candlestick(x=df_subset.index,\n                                 open=df_subset['Open'],\n                                 high=df_subset['High'],\n                                 low=df_subset['Low'],\n                                 close=df_subset['predicted_close'],\n                                 name='Candlesticks'),\n                  row=1, col=1)\n    \n    fig.add_trace(go.Scatter(x=df_subset.index, y=df_subset['pointpos'], mode=\"markers\",\n                             marker=dict(size=10, color=\"MediumPurple\", symbol='circle'),\n                             name=\"Entry Points\"),\n                  row=1, col=1)\n    \n    fig.update_layout(\n        width=1200, \n        height=800, \n        plot_bgcolor='black',\n        paper_bgcolor='black',\n        font=dict(color='white'),\n        xaxis=dict(showgrid=False, zeroline=False),\n        yaxis=dict(showgrid=False, zeroline=False),\n        showlegend=True,\n        legend=dict(\n            x=0.01,\n            y=0.99,\n            traceorder=\"normal\",\n            font=dict(\n                family=\"sans-serif\",\n                size=12,\n                color=\"white\"\n            ),\n            bgcolor=\"black\",\n            bordercolor=\"gray\",\n            borderwidth=2\n        )\n    )\n    \n    fig.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T19:03:27.356736Z","iopub.execute_input":"2024-12-17T19:03:27.357087Z","iopub.status.idle":"2024-12-17T19:03:27.372371Z","shell.execute_reply.started":"2024-12-17T19:03:27.357052Z","shell.execute_reply":"2024-12-17T19:03:27.371754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T19:03:27.376383Z","iopub.execute_input":"2024-12-17T19:03:27.376631Z","iopub.status.idle":"2024-12-17T19:03:27.406287Z","shell.execute_reply.started":"2024-12-17T19:03:27.376608Z","shell.execute_reply":"2024-12-17T19:03:27.405483Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = add_total_signal(df)\ndf = add_pointpos_column(df, \"TotalSignal\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T19:03:27.4074Z","iopub.execute_input":"2024-12-17T19:03:27.407749Z","iopub.status.idle":"2024-12-17T19:05:45.221353Z","shell.execute_reply.started":"2024-12-17T19:03:27.407721Z","shell.execute_reply":"2024-12-17T19:05:45.220365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = []\nheatmaps = []\n\ncols = ['open','high','low','predicted_prices','volume']\n\ndata = df[cols]\n\ndata=data.rename(columns={\"open\":\"Open\",\"low\":\"Low\",\"predicted_prices\":\"Close\",\"high\":\"High\",\"volume\":\"Volume\"})\nprint(data.info)\n\nbt = Backtest(data, MyStrat, cash=5000, margin=1/5, commission=0.0002)\nstats, heatmap = bt.optimize(slperc=[i/100 for i in range(1, 8)],\n                             tpperc=[i/100 for i in range(1, 8)],\n                maximize='Return [%]', max_tries=3000,\n                    random_state=0,\n                    return_heatmap=True)\nresults.append(stats)\nheatmaps.append(heatmap)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T19:06:51.274651Z","iopub.execute_input":"2024-12-17T19:06:51.275462Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"agg_returns = sum([r[\"Return [%]\"] for r in results])\nnum_trades = sum([r[\"# Trades\"] for r in results])\nmax_drawdown = min([r[\"Max. Drawdown [%]\"] for r in results])\navg_drawdown = sum([r[\"Avg. Drawdown [%]\"] for r in results]) / len(results)\n\nwin_rate = sum([r[\"Win Rate [%]\"] for r in results]) / len(results)\nbest_trade = max([r[\"Best Trade [%]\"] for r in results])\nworst_trade = min([r[\"Worst Trade [%]\"] for r in results])\navg_trade = sum([r[\"Avg. Trade [%]\"] for r in results]) / len(results)\n\n\nprint(f\"Aggregated Returns: {agg_returns:.2f}%\")\nprint(f\"Number of Trades: {num_trades}\")\nprint(f\"Maximum Drawdown: {max_drawdown:.2f}%\")\nprint(f\"Average Drawdown: {avg_drawdown:.2f}%\")\nprint(f\"Win Rate: {win_rate:.2f}%\")\nprint(f\"Best Trade: {best_trade:.2f}%\")\nprint(f\"Worst Trade: {worst_trade:.2f}%\")\nprint(f\"Average Trade: {avg_trade:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T19:05:45.407669Z","iopub.status.idle":"2024-12-17T19:05:45.40799Z","shell.execute_reply.started":"2024-12-17T19:05:45.407833Z","shell.execute_reply":"2024-12-17T19:05:45.407858Z"}},"outputs":[],"execution_count":null}]}